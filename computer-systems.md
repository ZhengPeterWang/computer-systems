# CS 3410

百年汪大传承知识火炬，造福一代代汪铮人民。

本文档为CS 3410预习的笔记。这门课主要讨论计算机系统的构成，以及处理器、内存和并行是如何工作的。本课程架起了硬件和软件之间的桥梁。

基础计算机可以看成由两个部分组成：内存(memory)和处理器(processor)。Central Processing Unit（CPU）就是现代电脑的处理器，而Random Access Memory(RAM)就是现代电脑的内存。处理器内部也有一些内部存储空间，称为寄存器(register），而处理器就相当于一个图灵机，不停地在不同状态间切换并修改寄存器的值，从而完成计算。内存则提供了远比处理器多的存储空间。两者是通过总线(bus)联系起来的。

为了与硬件交互，我们需要一种把内存和处理器抽象的平台，这样我们就能站在软件的角度理解硬件。这个平台叫指令集架构(Instruction Set Architecture)。

## 第1章 什么是电脑？电脑是怎么工作的？

目前的电脑主要有四种：个人电脑(PC)，服务器(server)，嵌入式微处理器（代表为单片机），和个人移动设备(PMD)。

### 1.1 电脑硬件简介

一个计算机主要由五个部分组成：**输入设备**（比如麦克风，键盘，鼠标），**输出设备**（比如音响，显示器，触摸屏），**内存，数据通路**(datapath)和**控制中心**，其中后两个经常并起来称为**处理器**。

#### 输入和输出设备

##### 图形显示

现代电脑是用二进制数字存储图像的。每个图像都被转成RGB格式表示的像素，每个像素都有三个字节，分别表示红色、绿色、黑色。数以万计的像素形成了显示器。现代的显示器大都是液晶显示器，是利用液晶分子通电后会改变其偏振属性，从而改变偏振光光强的方法控制不同单色光强度，从而显示出不同颜色的。现代电脑控制显示图片的工具是帧缓冲器(frame buffer)，需要显示的位图(bit map)先存储到帧缓冲器，再按照刷新速率不断地把信息输送给显示器。

##### 触摸设备

现代移动设备大量采用了电容感应方案。绝缘体玻璃被人体（导体）触摸后会改变屏幕的电场进而改变其电容，从而能够让系统电路获得感应信息。当然键盘等传统的靠压力通断电路开关的触摸设备现仍然大量地在个人电脑中使用。

#### 处理器

处理器又称**中央处理单元**(CPU)，主要包括控制中心和数据通路。数据通路是完成全部计算的主要设备，而控制中心则负责解析需要完成哪些计算。处理器一般被安放在**芯片**(chip)上。我们将在之后的更多章节讨论处理器。

#### 内存

现代的内存主要以**随机存取内存**(RAM)为形式。内存主要有动态随机存储内存（DRAM）和静态随机存储内存（SRAM，又称缓存,cache）。SRAM比DRAM更贵，内存容量更小，但更容易访问。现代电脑运用了多种不同的内存形成一套内存等级，以便最大限度地利用所有资源。

学习之后的课程后就能知道，**临时内存**（volatile memory)主要是通过控制电路某部分电压是否大于阈值来实现的。一旦断电，所有临时内存就都会丢失。因此，还有另一种内存称为**永久内存**(nonvolatile memory)，把信息用物理的方式存起来。临时内存因此又称主内存，而永久内存又称副内存。

永久内存主要有磁盘（magnetic disk，又称硬盘,hard disk, hard drive）和闪存(flash drive)。磁盘由盘片和磁头组成。盘片上布满了磁性物质。当我们向磁盘写信息时，磁头会去改变硬盘上某个位置的磁极。当我们向硬盘读信息时，磁头会去感应硬盘上某个位置的磁阻。为了读写盘片上的某个位置，磁头和盘片必须不停高速旋转以找到其对应的内存，这个过程会散发大量的热。磁盘内部还有一个微处理器，负责把读到的内容传递给数字电路。闪存则用电容存储信息。当闪存中的浮门带电时，闪存就拥有高电压，因此1就会被存储在这个晶体管。

内存全名 | 内存简称 | 存取时间 | 1GB存储的价格（2012）
-|-|-|-
静态随机存取内存 | SRAM | 10ns | $4k
动态随机存储内存 | DRAM | 50ns | $5 - $10
磁盘 | 硬盘 | 5 - 20 ms | $0.05 - $0.1
闪存 | flash | 0.1 ms | $1

我们将在之后的更多章节讨论内存，尤其是临时内存。

#### 网络

网络是现代电脑必不可缺的部分。它能够允许信息交互、资源共享、远程控制。网络有两种规模：局域网(local area network, LAN)和广域网(wide area network, WAN)。现代局域网速可以达到1-40GB/s，而通过光纤宽带实现的广域网速业已达到1GB/s。以wifi标准和通信网络为基础的无线网现在成为全新的网络形式，目前一般的网速可以达到100MB/s。

### 1.2 性能

电脑的性能可以用响应时间（延迟）和数据吞吐量描述。响应时间分为CPU时间和获得内存、IO等的时间，而CPU时间又分为系统CPU时间和用户CPU时间。我们之后主要讨论CPU时间，尤其是用户CPU时间。

决定性能的唯一标准是执行时间，但在不同的CPU上不同的程序执行时间有很大差别。一个程序的执行时间由指令个数、每指令执行的钟表圈数(CPI)、以及单位钟表圈数执行时间决定。一定要控制变量，才能够看出某个CPU执行某个任务的性能优劣。

为了提升性能，我们需要知道Amdahl定律，它强调，改变硬件或软件中某一部分的性能，并不能影响其他部分的性能，因此总性能提升受到提升性能是否经常被使用的影响。

### 1.3 指令集架构（电脑架构）

抽象性原理要求我们在研究更抽象内容时忽略具体实现细节，只需要把更具体的内容封装抽象为一个个模型即可。现代电脑最重要的一个抽象就是硬件与最低级软件之间的交互，这个交互模型称为**指令集架构**(Instruction Set Architecture)，简称**架构**(architecture)。这个抽象的架构是一套规则组成的要求，而具体的硬件应该*实现*这个要求。

在第3章中介绍处理器、第4章中介绍内存时，我们将更详细地讨论指令集架构。

## 第2章 电脑的电路基础

电脑是一个非常复杂的系统，但它的核心——处理器和内存——也不过是由大量三极管组成的一个电路。本章主要讨论一个个电路元件是如何被层层聚合，层层抽象，从而构成计算的基本单位和存储的基本单位的，它们将会成为处理器和内存的基础。

### 2.1 逻辑门电路

电路中把开关串联和并联可以构成一系列门电路。而NPN型与PNP型三极管则可以通过控制基极电压大小来控制电路通断，因此它们的恰当组合也能形成门电路（只不过它们只能形成非门和或非门），从而数字电路的通断不再需要开关的机械控制。在具体实现中我们不去管门电路如何实现，而只是把它抽象成一个可以满足我们要求的电路元件，这是从电路到电脑的第一层抽象。

电路与布尔代数表达式和命题逻辑表达式之间有一一对应的关系。任何电路都有一个真值表，能用合取范式设计出来。这是门电路被广泛应用于逻辑设计的原因。选择器(multiplexor)是一种重要的门电路。

一个重要的问题是，给出一个真值表，能否造出一个满足该真值表的表达式，使其运用最少的逻辑运算符（最少的门电路器件）。Karnaugh Maps是一种可以寻找该表达式的方法。

### 2.2 算术计算

以不同方式组合门电路，可以实现算术计算，这就是电脑被称为计算机的原因。

通过门电路可以设计一个带进位功能的全加法器，大致由6个门电路组成。加法器与加法器之间可以串联，这样理论上就可以设计出任意比特的加法器。用“2补法”构造每个正数的相反数可以构造与无符号整数兼容的有符号整数，这样就可以把加法运算的范围扩充到负数。减法就是加上相反数，因此减法也能用加法器串联组成，且能够与加法运算共用。8个比特的整数加法这样需要50个门电路左右就能完成了。注意一般比特的数目有限，因此大数字会导致计算越界，越界时可以通过一个选择器侦查结果符号是否正确，并抛出异常。

乘法、除法、甚至浮点数的计算，都能通过恰当的表示方式用比特和门电路的方式完成。当然门电路也能被用来比较数字大小。通过右移左移比特的方式还能完成乘方和开方。这样所有常用的算术计算都能用门电路完成了。

### 2.3 寄存器、内存

如何在一个瞬息万变的电路中存储什么信息呢？当然可以用电容是否带电的方式完成。但事实上只用门电路就可以实现存储信息。门电路可以组成多种多样的电器元件，这是在电脑组成中的第二层抽象。

SR锁存器(latch)是第一个可以存储一比特并改变其值的门电路。但SR锁存器中有一个态被禁止。D锁存器改进了SR锁存器，其中不再有禁止态，但D锁存器中存储的信号随外加信号的大小改变，也就是我们还是无法控制是修改这个比特还是存储这个比特。

为了实现周期性地决定是否存取某个内存，我们需要构造一个钟。电路中的钟就是周期信号，在本课程中我们使用正负边触发型钟，它是由电平的突然上升和下降来触发信号的。其他的还有电平触发型钟等。把两个D锁存器串联，再给它们接入相反的钟表信号，就造出了一个D触发器(flip-flop)。D触发器的特点是接受D信号，且只在钟信号突然上升或下降时才会根据D信号修改其内部存储的比特。这样11个门电路就能造出一个D触发器，它就能根据钟信号来存储一个比特。

将大量D触发器并联，分享一个钟信号，就构成了一个寄存器(register)，例如32比特寄存器，这是第三层抽象。大量的寄存器又组成了一个叫寄存器文件(register file)的单元，通过与编码器(encoder)和解码器(decoder)的组合可以实现写内存，通过与选择器的组合可以实现读内存。在寄存器文件中存储非常快速，只经过几个门电路就能完成，但它要实现大内存存储很困难，因为造不出太大的选择器。

存储大内存需要对寄存器文件改进。可以通过三态缓冲器（tri-state buffer）控制一个时间只有一个寄存器输出，且输出到一条所有寄存器共享的总线(bus)中去，这样就没必要造特别大的选择器了。利用这个思想可以造出静态随机存储内存(SRAM)。大量触发器可以通过二维的方式组织，总线就是word line和bit line，每轮钟表周期只读或写一个内存，这又是一层抽象。通过优化触发器可以让6个三极管就能存储一个比特，实现高效存储。

更高效的存储还可以通过DRAM来实现，它可以利用电容存储，但其存储速度比SRAM慢。可以看出，门电路构成了所有临时内存的框架。

### 2.4 顺序逻辑、有限状态机

在形式语言与自动机中我们已经讨论过有限状态机。把寄存器文件与带输入输出的组合逻辑电路（已经在2.1节讨论过）结合起来，就构成了一套顺序逻辑电路，也就能实现有限状态机。这样，门电路就提供了一套极其丰富的计算工具。

## 第3章 处理器

在拥有计算工具和内存之后，我们下一步将会考虑如何把它们组织起来实现指令。一个重要的经验是，指令（程序）也应该像数据一样存起来（存在寄存器文件中），只不过应该与数据分开存储。这样我们就能拥有更灵活的执行指令的工具（没有这个功能那么GOTO、递归、函数调用等等都将称为不可能）。本章中，我们以RISC处理器为例，阐述一套处理器的指令集架构是如何实现的，并讨论处理器的流水线优化。

### 3.1 RISC汇编语言

汇编语言(assembly language)是处理器的语言，也是指令集架构的语言。x86，ARM等是常用的汇编语言。本节我们既会讨论RISC汇编语言，也会讨论如何用上章的门电路元件实现这个语言的要求。

RISC类汇编语言的指令有算术逻辑计算（加减乘除左右移）、内存存取、控制执行语句等。指令的词法有4种：R型、I型、S型、U型等。

RISC类汇编语言可以用RISC-V处理器实现。这个处理器分五部分：获取指令、解码指令、执行、内存获取、回写寄存器。每个指令都会一次走过处理器的五个部分：第一步，从程序内存中读取要执行的指令。第二步，从寄存器文件中读取需要处理的临时内存。第三步，在ALU（算数逻辑单元）中执行指令。第四步，修改数据内存中的相应位置。第五步，把临时变量的更改结果重新存到寄存器，准备执行下一个指令。

算术与逻辑计算是比较简单的部分，只需要利用ALU就可以了。特别注意的是有中间值的算术计算，需要经过额外处理在其他地方传输中间值信号，还要注意中间值必须增位才能达到32位的寄存器存储值。

内存的获取和更改需要不停地读写SRAM或者DRAM或者其他。一般需要指明地址然后输入内存就可以了。注意RISC-V和x86认为小的数应该放在靠前的内存，而MIPS则认为大的数应该放在靠前的内存，这两者无优劣之分。

为了实现跳步执行，在寄存器获得某些信息后控制中心应该把运算结果发回一个加法器，控制程序内存从而改变下一个调用的语句。RISC还支持按照某条件进行比较，确认下一步应执行哪一行指令，这样我们就设计完成了能实现RISC语言的RISC-V处理器的基本框架。

在汇编语言实现中，函数调用是值得特别注意的。在硬件上为了实现在函数调用时寄存器与内存的资源利用最大化，约定了很多规则：把返回值在内存中的地址ra存在寄存器x1中；在寄存器sp（x2）中存着内存中调用栈指针（地址），方便递归；一般来说，函数的前8个参数存放在寄存器的10-17号地址，其他参数被放在栈中；实现RISC的操作系统的内存中栈从高往低，堆从低往高安排分配；全局变量可以专门在寄存器中存放一指针gp来在内存中找到它们去调用；在程序运行中要调用函数必须把自己的临时变量都存到栈中去，之后把它们调回来，被调用函数同理。

### 3.2 流水线优化

我们以上没有讨论处理器每条指令需要多长时间才能执行。单圈处理器的缺点是每个钟周期都只能执行一条指令，每个钟周期的长度不一样。如果优化使每个钟周期可以执行多条语句的话，我们就需要多圈处理器，即多个钟周期执行一条指令，比如每个钟周期只执行RISC-V中五个步骤中的一个，这样能确保每个钟周期长度基本相同。但这样每指令执行的圈数(CPI)就会远大于1，会导致处理器性能浪费。这时我们就要采用流水线方式来优化多圈处理器的执行。

流水线优化的意思是，当指令1走到第2步时，第1步对应的处理器部分也没有闲着，开始处理指令2。当指令1走到第3步时，指令2走到第2步，指令3走到第1步。这样，每个钟周期处理器的每个部分都没有闲着，一下子就会把CPI拉回到1左右，大大加快指令的执行速度。

但流水线优化有显然的危险之处。在一族指令中一个依赖于另一个的情况下，前一指令未完成后一指令就要获得某些前一指令还未来得及修改的数据，就会导致危险(hazard)。流水线优化主要有3种危险：结构危险、数据危险、控制危险。数据危险是指前一指令寄存器还未写入（第5步）后一指令就要使用它的值（第2步）。可以让后一指令等待前一指令执行完再继续，但这样效率较低。也可以让前一指令计算完就把未能来得及存到寄存器的临时变量转交给后一指令，这样能效率更高地解决数据危险。控制危险是指在第5步还未确定下一步需执行哪个指令时后一指令就已经出发了。这种危险可以通过等待前一指令执行完成，或者预测下一指令可能是什么来实现。结构危险是指并行造成的对某些资源的争夺，类似锁的现象，我们不做讨论。

### 3.3 其他架构简介

x86是除了RISC以外另一种指令集架构，它是目前大多PC所用的架构。x86有大于一千条指令，非常复杂，算符可以指定非寄存器的内存，有大量访问内存的方式。但RISC只有约200条指令，所有算符都必须是寄存器，只有一种访问内存的方式。目前在移动设备中通用的架构是ARM，ARMv7是一个32位的兼有x86和RISC特点的架构，而ARMv8是一个特别像RISC的64位架构。

### 3.4 编译以后

程序语言是人与计算机硬件交互的工具。不同的语言采用了不同的交互方式。以C为例，编译器(compiler)先通过词法、句法分析、类型检查、转为汇编语言等，把程序编译出来。在程序被编译成汇编语言后，程序还会通过汇编器(assembler)生成二进制的对象文件，再通过连接器(linker)将其要调用的头文件结合起来，生成可执行文件，这时编译全过程才正式结束。当可执行程序需执行时，装载器(loader)会将程序载入内存，跳到第一条指令，开始执行该程序的进程。启动装载器是操作系统负责的任务。

## 第4章 内存

现代电脑的内存常常被设计成等级形式，这是由于一般越快的内存越贵，因此越少，越慢的内存越便宜，因此越多。然而，通过有效设计内存的结构，并且利用人们访问内存的规律去预测可能访问的内存，我们可以制造拥有大量且快速内存的幻象，还可以让不同程序都有独享同一块大内存的幻象。这就是缓存和虚拟内存的原理。

### 4.1 缓存

如果每次内存调用都去主内存，甚至永久内存读写的话，那么程序的运行就会难以想象的慢——去芯片以外的任何地方都要至少100个钟周期，去主内存更是要几百万个钟周期。事实上，大量的内存访问是有规律的：程序偏好访问已经访问过的内存和靠近已访问过内存的内存。为了加快内存访问，有必要利用局域化原理，在一次内存访问时就把芯片外的部分其他数据拉回芯片，这就是缓存(cache)的合理性。

缓存有两种：一种是直接对应型缓存，即一个地址只存取一个缓存块，每个缓存块有大小，代表一个地址中存储的数据数目。每个缓存块都有地址，地址也有大小，表示此缓存对应地址的前几位。有时缓存块还有标签，标签是缓存对应地址的中间几位。为了尽可能多地覆盖相邻的存储空间，利用空间局域化，缓存应*增大其块大小*。有时最近明明访问过某地址，却因其对应的缓存被另一地址覆盖而无法访问该缓存，这种现象称为冲突。为了减少冲突，缓存应增加其*结合性*，即若新缓存与旧缓存冲突，则新缓存以类似于哈希表的形式找到一个新地址存储，这就是另一种缓存——结合型缓存。

描述缓存性能的数据是丢失率，即缓存未覆盖的内存需求占总内存需求的比例。增加结合性可以减少冲突，进而减少丢失率，但却会增加判断某缓存是否击中的时间。增加块大小可以覆盖更多相邻地址，却也会增加冲突。增加缓存的容量（个数）可以减少丢失率，但也会增加判断击中时间。

为了最大限度地利用缓存，在设计电脑架构时可以把缓存也分成不同层。更接近寄存器文件的缓存层更快，结合性小，缓存块小，容量小。更远离寄存器文件的缓存层更慢，结合性高，缓存块大，容量大。

以上我们只讨论了读取内存时用缓存优化的问题。当我们要写入内存，且击中缓存时，也有两种优化策略，一是积极型，即每次都既向缓存，也向内存写入。二是懒惰型，即只有当要清空本缓存时，才会把旧数据向内存写入。懒惰型比积极型更高效，写入次数更少，但在遇到并行和多核处理器时会有问题，会需要不停地潜入其他内存，保证所有缓存都一样，难以处理。

### 4.2 虚拟内存

如何同时运行多个程序？一个处理器一个时刻只能运行一个程序，所以严格地说只有多个处理器同时运行才能达到要求。但多个处理器如何作用才能不把内存搞乱呢？内存管理单元(MMU)可以给每个程序一个假象，即它占有全部的内存。当程序要内存时，它找内存管理单元要，内存管理单元给它一个内存，却不一定真正是它想要的那个地址。而内存管理单元只需要维护一张从真正内存到虚拟内存的表就行了。

为了方便内存虚拟化，我们把内存按页分配，每一内存页有4kB内存。一内存页最多时候只占一个进程。内存管理单元这时只需要负责胡乱地分配各个页就可以了。

虚拟内存可以提高内存存储效率，方便内存的合理分配。虚拟内存还可以把永久内存的存储空间搬过来，形成内存比实际临时内存还大的假象。但虚拟内存也有缺点，主要缺点是对于需要远小于4kB的进程来说虚拟内存会导致内存浪费。另外存储这张地址表的花销也很大，对于4GB内存来说可能需要4MB。更令人难以忍受的是，一个进程就需要一张地址表。当几百个进程同时运行时，光地址表就快要把临时内存耗光了。解决的办法是让每张地址表都不完整——程序要用一页内存就在地址表中添上一页的地址。这样地址表就很像缓存了——很可能一次性无法在地址表中找到页码，这个过程称为page fault。由于page fault一般意味着程序需要访问新的内存（很可能储存在永久内存上），page fault一般由操作系统通过系统调用（5.1节会讲到）处理。

找寻地址是否在地址表中存在（即是否有page fault）是非常耗时的工作。为了加速虚拟内存的地址分配，可以利用缓存，即翻译前视缓冲器（TLB），来提升判断是否有page fault过程的性能。为了找寻地址，程序需在TLB中先看看地址表有没有需要的页码。如果有，则在地址表中特定的位置寻找地址，如果没有，就进行系统调用，慢慢地从永久内存中读取数据。

### 4.3 其他存储介质

永久存储介质的发展非常快，从十年前以磁带为基础介质到现在以大数据中心、云计算为存储中心，云成为了现代数据的载体。

当然传统的磁盘也在不断被优化以减小寻址时间，通过优化找寻数据的顺序来减少磁头的移动距离。甚至微机也被用于加速磁盘寻址。现在也有通过存储多个冗余磁盘以容错和加速的存储方式。

## 第5章 操作系统、并行与输入输出

本章我们从硬件的角度讨论对操作系统的支持，并指出硬件在操作系统处理异常、中断和输入输出时的重要作用。本章还会讨论并行编程，及硬件与软件应如何结合才能提升并行编程的效率。

### 5.1 系统调用

并发编程中经常需要多个程序（在操作系统课中称为**进程**,process）共用一套硬件。操作系统是负责让多个进程各司其职不乱来的一个进程，因此操作系统有管理进程的特权。为了确保操作系统的特权，在硬件上可以给所有进程加上一个权限比特，而只有操作系统的权限比特才是1，这样就实现了系统资源的安全分配。

当然很多进程要输入输出，有的还要网络连接等，这些都是超出其权限的操作。这时它们可以通过系统调用机制，让操作系统帮忙实现输入输出、网络连接等。如果其要求过分，比如说要求占用太多内存的话，操作系统可直接把它们关闭。这样就保证了程序运行安全。

系统调用只是一种进程运行中的异常。其他异常还有错误、中断、电量不足和信息丢失等。硬件需要提供异常处理的支持。硬件在遇到异常时，应存储执行的指令，存储造成异常的原因，并把控制权交给操作系统。操作系统内核则会进行检查，保存寄存器，并尝试恢复硬件运行。

### 5.2 并行和多核编程

现在硬件的发展已经遇到了物理大小和能量功率的瓶颈，可对计算机性能优化的追求远没有停止。目前最重要的一个优化思路是并行(synchronization)，即在硬件上有多个完全相同的处理器作为*内核*，通过恰当的程序设计让多个内核负担均等，从而成倍加速程序执行。

并行编程的基本概念是**线程**(thread)。一个进程可以包含多个线程，它们共享代码、数据、文件，但每个线程有各自的寄存器和调用栈。随着并行概念的产生，兴起了超线程硬件的概念。超线程可由以下两种模式实现：（1）多发射处理器（即每个钟周期执行多条指令）加上额外寄存器；（2）多核处理器，并恰当处理危险。超线程硬件可在一个核上造出多核的假象，并且把流水线尽可能塞满，从而极大地提高程序执行效率。

从硬件角度，并行最大的问题是各个内核的缓存各自为政，无法自恰。解决方案是共享内存多处理器(SMM)。多核的缓存以某机制互相联系，而只有通过这个机制才能获得真正内存和输入输出，即所有内核都只有一套物理地址空间。但这并不能完全解决多线程缓存矛盾问题，无论缓存的优化策略是积极型还是懒惰型。

所谓自恰，是指读到的必须是最近写入的，且同一个内存地址在所有缓存中一个时刻只能有一个值。为此，数据必须及时从内存中移到所有的缓存。一种策略是，所有缓存都有个监视器监控总线，脏读时必须回应，被写入时必须更新，读入和写入时则不停地广播其他缓存持有的数据无效，必须重新从内存中读取。经过优化后的策略类似于在硬件角度加锁，即要么允许多读，要么只允许一读一写。即使如此硬件上的缓存自恰仍是很困难的问题，还要解决虚假分享内存（即分享内存块但没有分享具体内存）的问题。

没有程序设计的配合，并行是无法奏效的。即使硬件支持对缓存自恰的优化，程序也必须声明某些任务必须绑定顺序执行，否则多线程编程仍然会出问题，比如竞争执行等。我们定义，程序正确当且仅当将其所有可分部分按所有不同顺序执行时程序都正确。因此，程序必须自己把需要做的工作分成多个线程，每个线程绑定一批任务，允许多线程分开执行，并在必要时强制一个时刻只能有某个线程执行，即为线程上锁（互斥锁）。当锁得不到时，则做死循环，这种锁称为自旋锁。硬件则通过支持LR和SC来有效实现自旋锁。

在多线程程序设计时，我们必须恰当利用锁。读取和写入共享内存都必须加锁。一种要避免的情况是死锁，这要求我们必须按某个顺序加锁，即给锁优先级。另一个重要的机制是要求操作系统提供wait函数查看某个条件是否满足，只有当条件满足时才会尝试获得锁，否则会继续等待，这样既省去了死循环的资源浪费，又保证不会在尝试获得锁后条件又不满足了。

并行安全的数据结构称为监视器，它能够提供一把锁，某些条件变量和类似wait的函数，其中所有的行为都必须上锁。因此多线程编程安全最终是由软件的上锁解决的。

### 5.3 输入和输出

不同的输入输出设备效率不同，但与人交互的输入输出设备效率一般比处理器和内存的效率都低得多。为了不减慢硬件速度，从硬件上就应该把输入输出、读取内存等操作较慢的步骤分开管理，并与系统其他的高性能部分分离。总线在这个分离过程中起到了重要的作用。总线的参数有带宽、传输速度，以及是否允许并行等。处理器与内存之间的总线速度快，距离短，带宽大，而输入输出的总线则更长、更慢、更窄。由于不同输入输出方式的差别很大，总线机制可能不太高效，点对点的数据传输成为新模式。

如何让输入输出设备与程序交互呢？程序可通过系统调用启动输入输出。为了传输输入输出的数据，操作系统可指定某些内存地址的访问被重定向到输入输出设备的API，这个API是由命令寄存器、状态寄存器和数据寄存器组成的。这样输入输出数据的写入和读取也可以利用原有的虚拟内存处理方法，即重用缓存和虚拟内存等这些机制。这种机制称为直接内存访问输入输出(DMA)。DMA比起另一种机制，即构造一个软件程序，使得所有进程都通过这个程序与输入输出交互的机制更加高效。

输入输出设备在完成其任务后应该通过系统调用，向程序发送中断信号，而程序需要通过设置handler去处理这些不可预知的输入输出信息。一般来说这种方式比起每个钟周期都检查有没有输入输出来更高效。
